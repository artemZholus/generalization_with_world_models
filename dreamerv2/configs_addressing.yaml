defaults:

  # Train Script
  logdir: /dev/null
  seed: 0
  gpu: 0
  task: dmc_walker_walk
  train_tasks_file: none
  test_tasks_file: none
  randomize_tasks: false
  env_params: ''
  num_envs: 1
  parallel: False
  iid_eval: False
  steps: 4e6
  eval_every: 1e5
  eval_eps: 1
  action_repeat: 1
  time_limit: 0
  prefill: 10000
  image_size: [64, 64]
  grayscale: False
  keep_ram: False
  img_channels: 3
  replay_size: 2e6
  dataset: {batch: 50, length: 50, oversample_ends: True}
  replay: {rescan: 5000, cache: 100}
  precision: 16
  jit: True

  # Agent
  log_every: 1e3
  train_every: 5
  train_steps: 1
  pretrain: 0
  clip_rewards: identity
  expl_noise: 0.0
  expl_behavior: greedy
  expl_until: 0
  eval_noise: 0.0
  eval_state_mean: False
  agent_path: none
  wm_path: none
  ac_path: none

  # World Model
  world_model: dreamer
  pred_discount: True
  zero_shot: False
  transparent: False
  split_decoder: False
  train_wm_only: False
  train_ac_only: False
  grad_heads: [image, reward, discount]
  rssm: {hidden: 400, deter: 400, stoch: 32, discrete: 32, act: elu, std_act: sigmoid2, min_std: 0.1}
  encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [image], rect: False}
  decoder: {depth: 48, act: elu, kernels: [5, 5, 6, 6]}
  reward_head: {layers: 4, units: 400, act: elu, dist: mse}
  obj_gt_head: {layers: 2, units: 100, act: elu, dist: mse}
  discount_head: {layers: 4, units: 400, act: elu, dist: binary}
  loss_scales: {kl: 1.0, subj_kl: 1.0, obj_kl: 1.0, util_kl: 1.0, reward: 1.0, discount: 1.0}
  kl: 
    free: 0.0
    forward: False
    balance: 0.8
    free_avg: True
    subj: {free: 1.0, forward: False, balance: 0.5, free_avg: True}
    obj: {free: 1.0, forward: False, balance: 0.5, free_avg: True}
    util: {free: 1.0, forward: False, balance: 0.5, free_avg: True}
  model_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6}

  # Obj-Subj World Model
  subj_strategy: 'instant'
  cond_model_size: 50 
  cond_kws: {discrete: 0}
  subj_rssm: {hidden: 400, deter: 400, stoch: 32, discrete: 32, act: elu, std_act: sigmoid2, min_std: 0.1}
  obj_rssm: {hidden: 400, deter: 400, stoch: 32, discrete: 32, act: elu, std_act: sigmoid2, min_std: 0.1}
  subj_encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [subj_image], rect: False}
  obj_encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [obj_image], rect: False}
  obj_features: 'img'

  # Actor Critic
  actor: {layers: 4, units: 400, act: elu, dist: trunc_normal, min_std: 0.1}
  critic: {layers: 4, units: 400, act: elu, dist: mse}
  actor_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}
  critic_opt: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  actor_grad: both
  actor_grad_mix: '0.1'
  actor_ent: '1e-4'
  slow_target: True
  slow_target_update: 100
  slow_target_fraction: 1

  heads_feats: {subj: ['subj'], obj: ['obj'], policy: ['subj', 'obj', 'util'], reward: ['subj', 'obj', 'util']}

  zero_shot_agent:
    enabled: False
    train_every: 1

  # Multitask
  multitask: 
    data_path: none
    dataset: {batch: 50, length: 50, oversample_ends: True}
    mode: none
    bootstrap: False
    multitask_probability: .0
    multitask_batch_fraction: .0
  addressing: 
    kind: none
    hidden: 200
    optimizer: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}
    separate_enc_for_addr: True
    detach_cnn: False
    agent_only: False
    detach_task_embedding: False
    detach_multitask_embedding: False
    num_train_multitask_batches: 2
    num_query_multitask_batches: 2
    query_full_memory: True
    query: {layers: 0, units: 300, act: elu}
    key: {layers: 0, units: 300, act: elu}
    temp: 10.
    recalc_latent_freq: 200

  # Dyne
  embeddings:
    data_path: none
    traj_len: 4
    dataset: {batch: 128, oversample_ends: True}
    dyne:
      act_embed_size: 6
      obs_embed_size: 100
      action_net: {layers: 2, units: 400, act: selu, dist: logvar_normal}  
      obs_net_encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [image], rect: False}
      obs_net_dist: {layers: 0, units: 400, act: relu, dist: logvar_normal}
      recon_net: {layers: 2, units: 400, act: relu, dist: normal}
      image_net: {shape: [64, 64, 3], depth: 48, act: elu, kernels: [5, 5, 6, 6]}
    trainer:
      mode: none
      training_steps: 10000
      log_every: 10
      dyne_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6}
      action_kld_scale: 1e-2
      state_kld_scale: 1e-2

  # Exploration
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  expl_opt: {opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6}
  expl_head: {layers: 4, units: 400, act: elu, dist: mse}
  disag_target: stoch
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_action_cond: True
  expl_model_loss: kl

  # Logging
  logging:
    wdb: True
    run_name: none
    exp_name: none
    env_name: none

dual_v2:
  world_model: dual
  subj_rssm: {hidden: 200, deter: 200}
  obj_rssm: {hidden: 200, deter: 200}
  subj_strategy: 'instant'
  subj_encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [subj_image], rect: False}
  obj_encoder: {depth: 48, act: elu, kernels: [4, 4, 4, 4], keys: [obj_image], rect: False}

dual_v1:
  world_model: dual
  subj_rssm: {hidden: 200, deter: 200, stoch: 30, discrete: False, act: elu, std_act: sigmoid2, min_std: 0.1}
  rssm: {hidden: 200, deter: 200, stoch: 30, discrete: False, act: elu, std_act: sigmoid2, min_std: 0.1}
  obj_rssm: {hidden: 200, deter: 200, stoch: 30, discrete: False, act: elu, std_act: sigmoid2, min_std: 0.1}
  subj_strategy: 'instant'
  subj_encoder: {depth: 32, act: relu, kernels: [4, 4, 4, 4], keys: [subj_image], rect: False}
  obj_encoder: {depth: 32, act: relu, kernels: [4, 4, 4, 4], keys: [obj_image], rect: False}


v1:
  train_every: 1000
  train_steps: 100
  log_every: 1e3
  pred_discount: False
  grad_heads: [image, reward]
  rssm: {hidden: 200, deter: 200, stoch: 30, discrete: False, act: elu, std_act: sigmoid2, min_std: 0.1}
  encoder: {depth: 32, act: relu, kernels: [4, 4, 4, 4], keys: [image], rect: False}
  decoder: {depth: 32, act: relu, kernels: [5, 5, 6, 6]}
  reward_head: {layers: 2, units: 400, act: elu, dist: mse}
  discount_head: {layers: 3, units: 400, act: elu, dist: binary}
  loss_scales.kl: 1.0

  kl: {free: 3.0, forward: False, balance: 0.5, free_avg: True}
  model_opt: {opt: adam, lr: 6e-4, eps: 1e-5, clip: 100, wd: 0.0}

  critic: {layers: 3, units: 400, act: elu, dist: mse}
  actor_ent: 0.0
  actor_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 0.0}
  critic_opt: {opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 0.0}

  actor_grad: dynamics

  slow_target: False

addressing:
  multitask: 
    data_path: '$DATA/expert_buff/w_walk_600r_1000e/episodes'
    dataset: {batch: 51, length: 50, oversample_ends: True}
    mode: addressing
    bootstrap: False
    multitask_probability: 0.5 # expert_batch_prop
    multitask_batch_fraction: 0.5
  addressing:
    kind: reinforce_pred
    hidden: 200
    optimizer: {opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6}
    separate_enc_for_addr: True
    detach_cnn: False
    agent_only: False
    detach_task_embedding: True
    detach_multitask_embedding: False
    num_train_multitask_batches: 2 # number of multitask batches for training addressing
    num_query_multitask_batches: 2 # number of multitask batches to select from during training of the dreamer
    query_full_memory: False # whether to address over the full multitask memory 
                             # (equals to num_query_multitask_batches * multitask.dataset.length == total multitask transitions)
    temp: 50. # for same task experiment 50 was used; for multitask, it was 10
    recalc_latent_freq: 400

exp_w_stand:
  multitask: 
    data_path: '$DATA/multitask_buff/w_stand_1000e/episodes'

exp_w_walk:
  multitask: 
    data_path: '$DATA/multitask_buff/w_walk_600r_1000e/episodes'

exp_w_run:
  multitask: 
    data_path: '$DATA/multitask_buff/w_run_500r_1000e/episodes'

raw_multitask:
  multitask: 
    data_path: '$DATA/multitask_buff/w_walk_600r_1000e/episodes'
    dataset: {batch: 50, length: 50, oversample_ends: True}
    mode: raw
    multitask_probability: 0.5 # expert_batch_prop
    multitask_batch_fraction: 0.5

dyne_v1:
  dataset: {batch: 50, length: 52, oversample_ends: True}
  embeddings:
    data_path: '/data/expert_buff/w_walk_1000e/episodes'
    traj_len: 4
    dataset: {batch: 128, oversample_ends: True}
    dyne:
      act_embed_size: 6
      obs_embed_size: 100
      action_net: {layers: 2, units: 400, act: selu, dist: logvar_normal}  
      obs_net_encoder: {depth: 32, act: relu, kernels: [4, 4, 4, 4], keys: [image], rect: False}
      obs_net_dist: {layers: 0, units: 400, act: relu, dist: logvar_normal}
      recon_net: {layers: 2, units: 400, act: relu, dist: normal}
      image_net: {shape: [64, 64, 6], depth: 32, act: relu, kernels: [5, 5, 6, 6]}
    trainer:
      mode: sa_dyne
      training_steps: 10000
      log_every: 10
      action_kld_scale: 1e-2
      state_kld_scale: 1e-2
  multitask:
    data_path: '/data/expert_buff/w_walk_1000e/episodes'
    mode: addressing_dyne
    dataset: {batch: 50, length: 52, oversample_ends: True}
  addressing:
    detach_cnn: True
    separate_enc_for_addr: True


atari:

  task: atari_pong
  time_limit: 108000  # 30 minutes of game play.
  action_repeat: 4
  steps: 2e8
  eval_every: 1e5
  log_every: 1e5
  prefill: 200000
  grayscale: True
  train_every: 16
  clip_rewards: tanh
  rssm: {hidden: 600, deter: 600, stoch: 32, discrete: 32}
  actor.dist: onehot
  model_opt.lr: 2e-4
  actor_opt.lr: 4e-5
  critic_opt.lr: 1e-4
  actor_ent: 1e-3
  discount: 0.999
  actor_grad: reinforce
  actor_grad_mix: 0
  loss_scales.kl: 0.1
  loss_scales.discount: 5.0
  .*\.wd$: 1e-6

mtw:

  task: metaworld_ml1_reach
  img_channels: 6
  env_params: "{cameras: [corner, corner3], offscreen: True, randomize_tasks: True}"
  time_limit: 500
  action_repeat: 1
  eval_every: 1e4
  log_every: 1e3
  prefill: 5000
  train_every: 5
  pretrain: 100
  pred_discount: False
  grad_heads: [image, reward]
  rssm: {hidden: 200, deter: 200}
  model_opt.lr: 3e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 8e-5
  actor_ent: 1e-4
  discount: 0.99
  actor_grad: dynamics
  kl.free: 1.0
  dataset.oversample_ends: False

onscreen:
  env_params: "{cameras: [corner, corner3], offscreen: False, randomize_tasks: True}"

ml1:
  logging.exp_name: ml1
  logging.env_name: ml1
  logdir: $DATA/tmp_noroot/{run_id}
  steps: 1e6

rotated_drawer_close:
  logging.exp_name: ml1_rotated
  img_channels: 3
  env_params: "{cameras: [topview], offscreen: True, randomize_tasks: True}"
  eval_eps: 10
  num_envs: 10
  parallel: true
  train_every: 10
  train_steps: 2
  task: metaworld_ml1_drawer-close-rotated
  train_tasks_file: ./rotated_drawer_tasks/close_monotonic_train
  test_tasks_file: ./rotated_drawer_tasks/close_monotonic_test
  randomize_tasks: true

close_monotone:
  task: metaworld_ml1_drawer-close-rotated
  train_tasks_file: ./rotated_drawer_tasks/close_monotonic_train
  test_tasks_file: ./rotated_drawer_tasks/close_monotonic_test

open_monotone:
  task: metaworld_ml1_drawer-open-rotated
  train_tasks_file: ./rotated_drawer_tasks/open_monotonic_train
  test_tasks_file: ./rotated_drawer_tasks/open_monotonic_test

close_umbrella:
  task: metaworld_ml1_drawer-close-rotated
  train_tasks_file: ./rotated_drawer_tasks/close_umbrella_train
  test_tasks_file: ./rotated_drawer_tasks/close_umbrella_test

open_umbrella:
  task: metaworld_ml1_drawer-open-rotated
  train_tasks_file: ./rotated_drawer_tasks/open_umbrella_train
  test_tasks_file: ./rotated_drawer_tasks/open_umbrella_test

open_full:
  task: metaworld_ml1_drawer-open-rotated
  train_tasks_file: ./rotated_drawer_tasks/open_full_train
  test_tasks_file: ./rotated_drawer_tasks/open_full_test

close_full:
  task: metaworld_ml1_drawer-close-rotated
  train_tasks_file: ./rotated_drawer_tasks/close_full_train
  test_tasks_file: ./rotated_drawer_tasks/close_full_test

causal:
  transparent: True
  obj_features: 'img'
  grad_heads: [obj_image, subj_image, reward]
  critic.layers: 3
  critic_opt.wd: 0.000001
  decoder.act: elu
  decoder.depth: 32
  kl.balance: 0.5
  model_opt.lr: 0.0006
  obj_encoder.act: elu
  obj_encoder.depth: 32
  subj_encoder.act: elu
  subj_encoder.depth: 32
  rssm.hidden: 200
  rssm.discrete: 0
  actor.layers: 3
  reward_head.layers: 3
  # cond_model_size: 32
  # cond_kws.discrete: 16
  cond_model_size: 200
  cond_kws.discrete: 0
  iid_eval: True
  steps: 4e6
  world_model: causal

mt10:
  logging.exp_name: mt10
  logging.env_name: mt10
  task: metaworld_mt10_all
  logdir: $DATA/tmp_noroot/{run_id}
  steps: 1e8
  eval_eps: 10
  num_envs: 10
  parallel: True
  train_every: 10
  train_steps: 2
  env_params: "{cameras: [corner, corner3], offscreen: True, randomize_tasks: False}"

dmc:

  task: dmc_walker_walk
  time_limit: 1000
  action_repeat: 2
  eval_every: 1e4
  log_every: 1e3
  prefill: 5000
  train_every: 5
  pretrain: 100
  pred_discount: False
  grad_heads: [image, reward]
  rssm: {hidden: 200, deter: 200}
  model_opt.lr: 3e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 8e-5
  actor_ent: 1e-4
  discount: 0.99
  actor_grad: dynamics
  kl.free: 1.0
  dataset.oversample_ends: False

dmc_walker:
  logging.exp_name: walker_domain
  logging.env_name: run
  task: dmc_walker_run
  logdir: $DATA/tmp_noroot/{run_id}

dmc_hopper:
  logging.exp_name: hopper_domain
  logging.env_name: hop
  task: dmc_hopper_hop
  logdir: $DATA/tmp_noroot/{run_id}

debug:

  jit: False
  time_limit: 100
  # eval_every: 300
  # log_every: 300
  prefill: 100
  pretrain: 1
  # train_steps: 1
  dataset.batch: 12
  dataset.length: 50
debug2:

  jit: False
  # time_limit: 100
  # eval_every: 300
  # log_every: 300
  # prefill: 100
  pretrain: 1
  # train_steps: 1
  # dataset.batch: 12
  # dataset.length: 50
